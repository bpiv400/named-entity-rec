# Named Entity Recognition Homework
By Claire Donovan and Barry Plunkett
## Part 1: Constrained Task
Our constrained task was to produce a named entity recognition model, which could achieve $F_1 > .60$ on the Spanish test set without copying code from online sources or hijacking named entity taggers developed by other researchers.  We measured the performance of our model with a phrase-based F-score. Under this framework, the model needed to tag the entire named entity phrase correctly for it to be considered an accurate tag. In other words, the model needed to identify the beginning, middle, and end of a named entity to successfully tag it; no partial credit was given for anything else. 

The model identified four kinds of named entities separately: 
* PER: Person
* LOC: Location
* ORG: Organization
* MISC: Miscellaneous
For each type of entity, we also attempted to distinguish between the beginning of an entity (e.g. B-PER) and the "inside", meaning any token included in the named entity except the first, (e.g. I-PER).  The model assigned an "O" tag to non-entity tokens. 
### Feature Set
Below, we highlight some features we implemented, which we believed could store latent predictive signal about. We also attempt to describe our intuition for why each feature could be important: 
* Word: Simply the unprocessed word given by each token. The intuition for this feature's importance is straightforward: Some words are probably more likely than others to be named entities. For example, Pablo is probably more likely to be a named entity than truck. We included this feature for offset words in a symmetric two-token window  around the token in question. For this feature and for others, we believed including information about surrounding tokens could help classify ambiguous words that don't necessarily refer to named entities. For example, "Los", meaning "the" in spanish is probably not part of a named entitity unless the next word is "Angelos" or something similar. A similar argument can be made for looking at past words for tokens like "de", meaning of, since it's probably not part of a named entity unless the previous word was a name.
* Lemma: The dictionary form of a token. In other words, the token with inflectional endings and prefixes removed. Lemmas also remove the conjugations of verbs. For example, the lemma of "went" is "go." We determined lemmas for each word by referencing a list of 400,000 Spanish word-lemma tokens available [here](http://www.lexiconista.com/datasets/lemmatization/) We included lemmas in a two word symmetric window around the token in question for the same reason we included words: Some lemmas are probably more likely to be named entities than others. We didn't think lemmas would be redundant with words because we thought they might help the model generalize patterns across different tokens with the same lemmas. Additionally, we were concerned that having two sources of variance (syntax and semantics) might undercut the value of the word feature. We hoped including lemmas, which are primarily influenced by semantics, would help the model distinguish these signals. Based on the same intuition that generalizing across morphologies could help identify patterns among named entities, we also extracted __prefixes__ and __suffixes__ from each target word as features. 
* Part of Speech (POS): Part of Speech for each word in a 2-word symmetric window around the target token. We simply extracted POS from the NLTK corpus. We included POS because we believed words with certain parts of speech would be more likely than others to be named entities. For example, proper nouns are probably included in named entities more often than prepositions or conjunctions. We included this feature for a symmetric window around the current token because like our word and lemma heuristics, the information encoded by part of speech is likely context dependent. For instance, if a preposition is surrounded by two proper nouns, it's probably more likely to be in a named entity phrase than if the same preposition surrounded by a verb and a noun. 
* First Letter capitalized: This feature is pretty self-explanatory. We encoded whether the first letter of a word is capitalized as a simple 0-1 indicator feature for each word. We thought this would be useful because named entities tend to have the first letter of their words capitalized--names, places, and famous events all usually have capitalized words, for example. Of course, we thought the value of these feature might be undermined by words that are capitalized due to grammatical convention, even though they're not part of a named entity. In particular, we were concerned about words occuring at the start of sentences, so we also considered: 
    * An indicator feature for whether a word was at the start of a sentence
    * A numeric feature for the index of a token in a sentence
* Entire word Capitalized: Again, self-explanatory. We encoded whether the entire token was capitalized with an indicator feature. We thought this would be valuable because acronyms, which tend to assigned to government entities, firms, stock-tickers, and other official organizations are usually fully capitalized and refer to named entities. It's rare for words to be fully capitalized and not be named entities (unless they're one letter words at the start of a sentence).
### Algorithms
  Below, we highlight several of the algorithms we used to for our classifier and describe why we chose them: 
  * Unnregularized Perceptron: A simple baseline to compare more complex non-linear models to. Unregularized perceptron models efficiently find boundaries that separate linear data, so thought this would function as a valuable baseline. 
  * Multi-Layer Perceptron: A more complex model capable of learning non-learn functions. Depending on depth, MLPs can have far more weights than an ordinary perceptron, so they require more training data to produce an accurate model. Their capacity to learn non-linearities also makes them vulnerable to overfitting. As a result, we used cross-validation on the development set to test several different layer sizes (50, 100, and 200 hidden units) to combat overfitting. We also limited the number of iterations to less than 500, instead of stopping at a local minimum. This early cutoff should help reduce variance. 
  * Logistic Regression Classifier: Logistic regression assumes class probabilities of instances can be represented with a squashed general linear function, and selects the linear model that minimizes log loss over these functions. Since logistic regression is learning a linear model, we hypothesized that its results would be similar to the unregularized perceptron. There is one important distinction: We included $L_2$ regularization in the logistic regression objective function to reduce variance. We tested several values of inverse regularization strength (.01, .1, 1, 10, 100) with cross-validation. 
  * Adaboost: The adaboost algorithm learns a strong classifier from numerous weak classifiers. We thought this could be useful because in our mental model of the problem, each of features could function as a weak decision rule, which when combined with others, could be quite powerful. In confirmation of this hypothesis, we found that one of the papers from the shared task used a multi-class version of [adaboost](http://www.aclweb.org/anthology/W02-2024). For our weak classifiers, we simply used sets of decision stumps. We tried training 100 and 300 and 600 decision stumps and compared these with cross-validation. 
### Results
First, we used the baseline perceptron and logistic regression classifiers to compare combinations and select a few optimal combinations for the remaining classifiers. Here's a key for the following table: 
* C represents the inverse strength of the regularizer
 All: word, lemma, prefix, suffix, first-cap, all-cap, sentence index, first-word-of-sentence, part of speech, symmetric offset width of 2
* No_Sent: All without sentence index feature
* No_Affixes: All without  affix features
* No_cap: All without features without capitalization related features
* No_lemma: All features without lemmas
* All_narrow: extracts offset features only for 1 token window around word 
* All_wide: extracts offset features by 3 tokens window around word 

| Feature Set |            LogReg C = .01    |              LogReg C = .1|             LogReg C = 1  |                LogReg C = 10    |                  LogReg C = 100    |              Perceptron  |
| ----------- | ---------------------------- | --------------------------| ------------------------- | ------------------------------- | ---------------------------------- | ------------------------ |
| All         |            48.17             |              57.86        |             66.30         |                67.25            |                  66.02             |              43.40       |
| No_Sent     |            48.31             |              58.00        |             66.05         |                67.33            |                  65.84             |              52.67       |
| No_lemma    |            46.45             |              54.58        |             63.85         |                67.43            |                  66.26             |              55.23       |
| No_Affixes  |            47.51             |              57.28        |             65.61         |                66.72            |                  65.33             |              56.99       |
| All_narrow  |            42.71             |              53.98        |             62.06         |                64.85            |                  64.03             |              53.03       |
In general, some regularization seems to improve model performance. This is intuitive given the size of the data set in comparison to the sample space. With only 8000 sentences, we can't expect our sample to generalize well to other similarly small samples, when the syntactic and semantic variance of sentences is enormous. Removing the sentence index feature also seems to improve the performance of the unregularized model. This suggests most of the information encoded by sentence index is fairly noisy. As a result, we remove sentence index for the remaining feature sets. Removing affixes doesn't seem to have any affect. Removing lemmas and affixes don't seem to have any major affect on the model's performance either. Notably, the unregularized model improves significantly, which is consistent with the model overfitting noise in the affix and lemma dimensions. 
